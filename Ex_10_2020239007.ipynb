{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**This question should be answered using PyTorch and MNIST data set discussed in the class.**"
      ],
      "metadata": {
        "id": "W55uUhN17vVe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LgrGrPVH7Tn7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download training data from open datasets.\n",
        "training_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets\n",
        "test_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYgwDTaA78Ec",
        "outputId": "263093e1-b544-46ad-f044-415142f8552b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 45.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.76MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 13.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.50MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnW6ah2q8P6P",
        "outputId": "7848a93b-652e-4776-97ee-7cfc1a314331"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**a. Find a suitable configuration of Neural Network and fit the model to predict handwritten digits with high accuracy.**"
      ],
      "metadata": {
        "id": "GWXQKQl5BIQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get cpu, gpu or mps device for training.\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWyeOjYW8uP4",
        "outputId": "47b6acee-a731-4d8c-d3eb-6853b21bd4dd"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "u1L_Kf-18zuN"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ],
      "metadata": {
        "id": "gUdhWnHd83PI"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "WQDQq2Fk85nS"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQmXxLM089sx",
        "outputId": "835b6a77-2892-42ae-d09f-a55213798e1f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.308900  [   64/60000]\n",
            "loss: 2.300448  [ 6464/60000]\n",
            "loss: 2.300684  [12864/60000]\n",
            "loss: 2.282619  [19264/60000]\n",
            "loss: 2.287493  [25664/60000]\n",
            "loss: 2.279827  [32064/60000]\n",
            "loss: 2.270112  [38464/60000]\n",
            "loss: 2.277222  [44864/60000]\n",
            "loss: 2.255399  [51264/60000]\n",
            "loss: 2.248954  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 46.8%, Avg loss: 2.246060 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.245853  [   64/60000]\n",
            "loss: 2.233983  [ 6464/60000]\n",
            "loss: 2.249128  [12864/60000]\n",
            "loss: 2.202466  [19264/60000]\n",
            "loss: 2.223453  [25664/60000]\n",
            "loss: 2.214411  [32064/60000]\n",
            "loss: 2.191483  [38464/60000]\n",
            "loss: 2.218281  [44864/60000]\n",
            "loss: 2.174505  [51264/60000]\n",
            "loss: 2.160833  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 58.9%, Avg loss: 2.160034 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.159463  [   64/60000]\n",
            "loss: 2.137447  [ 6464/60000]\n",
            "loss: 2.170759  [12864/60000]\n",
            "loss: 2.083395  [19264/60000]\n",
            "loss: 2.120805  [25664/60000]\n",
            "loss: 2.106322  [32064/60000]\n",
            "loss: 2.064151  [38464/60000]\n",
            "loss: 2.117057  [44864/60000]\n",
            "loss: 2.042820  [51264/60000]\n",
            "loss: 2.015069  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 66.7%, Avg loss: 2.014541 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 2.016414  [   64/60000]\n",
            "loss: 1.975275  [ 6464/60000]\n",
            "loss: 2.030630  [12864/60000]\n",
            "loss: 1.888917  [19264/60000]\n",
            "loss: 1.940579  [25664/60000]\n",
            "loss: 1.917836  [32064/60000]\n",
            "loss: 1.851712  [38464/60000]\n",
            "loss: 1.940923  [44864/60000]\n",
            "loss: 1.824384  [51264/60000]\n",
            "loss: 1.775692  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 71.2%, Avg loss: 1.773266 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.784665  [   64/60000]\n",
            "loss: 1.711856  [ 6464/60000]\n",
            "loss: 1.791707  [12864/60000]\n",
            "loss: 1.599523  [19264/60000]\n",
            "loss: 1.652143  [25664/60000]\n",
            "loss: 1.622163  [32064/60000]\n",
            "loss: 1.540822  [38464/60000]\n",
            "loss: 1.676134  [44864/60000]\n",
            "loss: 1.522463  [51264/60000]\n",
            "loss: 1.455328  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 74.8%, Avg loss: 1.447604 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**b. Demonstrate prediction for some values.**"
      ],
      "metadata": {
        "id": "DnsejVlDBazj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Select a random subset of data points from the test set\n",
        "num_samples_to_predict = 10\n",
        "random_indices = random.sample(range(len(test_data)), num_samples_to_predict)\n",
        "\n",
        "# Create a subset of the test data loader for selected indices.\n",
        "subset_test_data = torch.utils.data.Subset(test_data, random_indices)\n",
        "subset_test_dataloader = DataLoader(subset_test_data, batch_size=1)\n",
        "\n",
        "# Perform predictions\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for X, y in subset_test_dataloader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred = model(X)\n",
        "        predicted_label = pred.argmax(1).item()\n",
        "        print(f\"Predicted label: {predicted_label}, Actual label: {y.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfA3ZhfABg99",
        "outputId": "991038ea-2c9b-45d6-ceb4-91a47107441e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted label: 7, Actual label: 5\n",
            "Predicted label: 7, Actual label: 7\n",
            "Predicted label: 3, Actual label: 3\n",
            "Predicted label: 9, Actual label: 9\n",
            "Predicted label: 1, Actual label: 1\n",
            "Predicted label: 8, Actual label: 8\n",
            "Predicted label: 4, Actual label: 4\n",
            "Predicted label: 8, Actual label: 8\n",
            "Predicted label: 1, Actual label: 1\n",
            "Predicted label: 0, Actual label: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**c. Print the possible Performance Metrics of the model.**"
      ],
      "metadata": {
        "id": "RL6CJ4QMCRSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "def get_predictions(dataloader, model):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            predicted_labels = pred.argmax(1).cpu().numpy()\n",
        "            all_predictions.extend(predicted_labels)\n",
        "            all_labels.extend(y.cpu().numpy())\n",
        "    return all_predictions, all_labels\n",
        "\n",
        "\n",
        "y_pred, y_true = get_predictions(test_dataloader, model)\n",
        "\n",
        "\n",
        "print(classification_report(y_true, y_pred))\n",
        "print(confusion_matrix(y_true, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rl2out1vCUcK",
        "outputId": "60b5d4f6-d75b-4377-9777-fb6d1d9e2ff1"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.98      0.81       980\n",
            "           1       0.76      0.99      0.86      1135\n",
            "           2       0.85      0.72      0.78      1032\n",
            "           3       0.63      0.84      0.72      1010\n",
            "           4       0.82      0.65      0.72       982\n",
            "           5       1.00      0.09      0.17       892\n",
            "           6       0.86      0.87      0.86       958\n",
            "           7       0.74      0.87      0.80      1028\n",
            "           8       0.79      0.64      0.71       974\n",
            "           9       0.67      0.72      0.70      1009\n",
            "\n",
            "    accuracy                           0.75     10000\n",
            "   macro avg       0.78      0.74      0.71     10000\n",
            "weighted avg       0.78      0.75      0.72     10000\n",
            "\n",
            "[[ 956    0    2    6    0    0   10    1    5    0]\n",
            " [   0 1119    6    3    0    0    4    1    2    0]\n",
            " [  67   93  745   36   19    0   26   22   24    0]\n",
            " [  28   17   28  852    0    0    9   31   35   10]\n",
            " [  12   26    4    0  634    0   23   10    4  269]\n",
            " [ 159   45   28  316   30   84   48   74   78   30]\n",
            " [  59   21   22    5    6    0  838    1    6    0]\n",
            " [  12   65   19    0    6    0    1  899    4   22]\n",
            " [  45   65   18  129   10    0   18   42  621   26]\n",
            " [  34   21    3   14   65    0    3  134    4  731]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Summary:-**\n",
        "\n",
        "The code trains a neural network on the MNIST dataset to classify handwritten digits.  Here's a summary of the inferences:\n",
        "\n",
        "1. **Model Architecture:** A simple feedforward neural network with two hidden layers (1024 and 512 neurons) and ReLU activation functions is used.  The input is flattened from a 28x28 image, and the output layer has 10 neurons (one for each digit). Any other combination of the layers and neurons were either slow or was not as accurate.\n",
        "\n",
        "2. **Training:** The model is trained using stochastic gradient descent (SGD) with a learning rate of 1e-3 and the cross-entropy loss function.  The training process involves iterating through the training data for 5 epochs, printing the loss at intervals, and evaluating performance on the test set after each epoch.\n",
        "\n",
        "3. **Evaluation:**  Accuracy and average loss are reported on the test set after each epoch.  A `classification_report` provides precision, recall, F1-score, and support for each digit class. A confusion matrix is also generated, showing the counts of correct and incorrect predictions for each digit class.\n",
        "\n",
        "4. **Prediction Demonstration:** The code randomly selects 10 samples from the test dataset and demonstrates the model's predictions on these samples.\n",
        "\n",
        "5. **Performance Metrics:**  The provided `classification_report` and `confusion_matrix` offer detailed insights into model performance. The classification report gives class-wise metrics. The confusion matrix provides a visualization of how often one digit is misclassified as another.\n",
        "\n",
        "\n",
        "In essence, the code trains and evaluates a basic neural network for handwritten digit recognition, provides examples of the model's predictions and evaluates the model's performance using common classification metrics.\n",
        "\n",
        "**The average accuracy of the model is about 75%.**(maximum possible for me)\n",
        "\n"
      ],
      "metadata": {
        "id": "WL-ef9sdC8Fq"
      }
    }
  ]
}