{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. Respective dataset file can be downloaded from: https://www.nseindia.com/reports-indices-historical-index-data**\n",
        "\n",
        "**2. The dataset in csv will have historical data of NSE indices.**\n",
        "\n",
        "**3. You shall download the recent last 365 days data for NIFTY 50 Index from the above link. A sample downloaded data is attached for reference.**\n",
        "\n",
        "**4. Develop a notebook to predict the Closing value of the said index for today (current day of running of the program) using RNN given the past 365 days data from today.**\n",
        "\n",
        "**5. Use PyTorch for your work. Refer for implementation documentation provided at: https://scikit-learn.org/stable/**\n",
        "\n",
        "Code should contain comments appropriately\n",
        "You may include appropriate indicators required, change data frame as required, use plots as required.**"
      ],
      "metadata": {
        "id": "2qmrEyjHfIzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Load and preprocess the data\n",
        "# Load your data\n",
        "file_path = \"/content/NIFTY 50-20-11-2023-to-20-11-2024.csv\"\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Strip whitespace from column names\n",
        "data.columns = data.columns.str.strip()\n",
        "\n",
        "# Convert the Date column to datetime format\n",
        "data['Date'] = pd.to_datetime(data['Date'], format='%d-%b-%y')\n",
        "\n",
        "# Sort the data by date\n",
        "data = data.sort_values('Date')\n",
        "\n",
        "\n",
        "\n",
        "# Normalize the 'Close' column\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "data['Close_scaled'] = scaler.fit_transform(data[['Close']])\n",
        "\n",
        "# Step 2: Create sequences\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, data, seq_length=365):\n",
        "        self.seq_length = seq_length\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.seq_length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index:index + self.seq_length]\n",
        "        y = self.data[index + self.seq_length]\n",
        "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "# Prepare the dataset\n",
        "seq_length = 30\n",
        "dataset = TimeSeriesDataset(data['Close_scaled'].values, seq_length)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Step 3: Define the LSTM model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=50, num_layers=2):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc(out[:, -1, :])  # Take the output of the last time step\n",
        "        return out\n",
        "\n",
        "# Step 4: Train the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = LSTMModel().to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.unsqueeze(-1).to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(X_batch)\n",
        "        loss = criterion(output.squeeze(), y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {train_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch, y_batch = X_batch.unsqueeze(-1).to(device), y_batch.to(device)\n",
        "        output = model(X_batch)\n",
        "        loss = criterion(output.squeeze(), y_batch)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "print(f\"Test Loss: {test_loss/len(test_loader):.4f}\")\n",
        "\n",
        "# Step 6: Make predictions\n",
        "with torch.no_grad():\n",
        "    last_365_days = torch.tensor(data['Close_scaled'].values[-seq_length:], dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)\n",
        "    predicted = model(last_365_days)\n",
        "    predicted_value = scaler.inverse_transform(predicted.cpu().numpy())\n",
        "\n",
        "print(f\"Predicted Closing Value for Today: {predicted_value[0][0]:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvk6slcEaLCX",
        "outputId": "4b430bd2-82f1-4271-a37d-68fb582a8144"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 0.2662\n",
            "Epoch 2/50, Loss: 0.1457\n",
            "Epoch 3/50, Loss: 0.0514\n",
            "Epoch 4/50, Loss: 0.0456\n",
            "Epoch 5/50, Loss: 0.0350\n",
            "Epoch 6/50, Loss: 0.0335\n",
            "Epoch 7/50, Loss: 0.0292\n",
            "Epoch 8/50, Loss: 0.0285\n",
            "Epoch 9/50, Loss: 0.0249\n",
            "Epoch 10/50, Loss: 0.0222\n",
            "Epoch 11/50, Loss: 0.0172\n",
            "Epoch 12/50, Loss: 0.0116\n",
            "Epoch 13/50, Loss: 0.0070\n",
            "Epoch 14/50, Loss: 0.0109\n",
            "Epoch 15/50, Loss: 0.0081\n",
            "Epoch 16/50, Loss: 0.0077\n",
            "Epoch 17/50, Loss: 0.0062\n",
            "Epoch 18/50, Loss: 0.0054\n",
            "Epoch 19/50, Loss: 0.0053\n",
            "Epoch 20/50, Loss: 0.0053\n",
            "Epoch 21/50, Loss: 0.0053\n",
            "Epoch 22/50, Loss: 0.0047\n",
            "Epoch 23/50, Loss: 0.0049\n",
            "Epoch 24/50, Loss: 0.0046\n",
            "Epoch 25/50, Loss: 0.0044\n",
            "Epoch 26/50, Loss: 0.0045\n",
            "Epoch 27/50, Loss: 0.0042\n",
            "Epoch 28/50, Loss: 0.0042\n",
            "Epoch 29/50, Loss: 0.0041\n",
            "Epoch 30/50, Loss: 0.0038\n",
            "Epoch 31/50, Loss: 0.0039\n",
            "Epoch 32/50, Loss: 0.0039\n",
            "Epoch 33/50, Loss: 0.0039\n",
            "Epoch 34/50, Loss: 0.0041\n",
            "Epoch 35/50, Loss: 0.0041\n",
            "Epoch 36/50, Loss: 0.0038\n",
            "Epoch 37/50, Loss: 0.0043\n",
            "Epoch 38/50, Loss: 0.0040\n",
            "Epoch 39/50, Loss: 0.0038\n",
            "Epoch 40/50, Loss: 0.0035\n",
            "Epoch 41/50, Loss: 0.0033\n",
            "Epoch 42/50, Loss: 0.0031\n",
            "Epoch 43/50, Loss: 0.0030\n",
            "Epoch 44/50, Loss: 0.0030\n",
            "Epoch 45/50, Loss: 0.0030\n",
            "Epoch 46/50, Loss: 0.0030\n",
            "Epoch 47/50, Loss: 0.0029\n",
            "Epoch 48/50, Loss: 0.0029\n",
            "Epoch 49/50, Loss: 0.0027\n",
            "Epoch 50/50, Loss: 0.0029\n",
            "Test Loss: 0.0032\n",
            "Predicted Closing Value for Today: 23986.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Process Followed**\n",
        "**Dataset Loading and Inspection:**\n",
        "\n",
        "Imported the dataset and cleaned the column names to handle any whitespace issues.\n",
        "Ensured the Date column was correctly parsed into a datetime object for sorting.\n",
        "Normalized the Close values using Min-Max scaling to prepare the data for training.\n",
        "Sequence Generation:\n",
        "\n",
        "Implemented the TimeSeriesDataset class to create input-output pairs for the Recurrent Neural Network (RNN), where each input sequence (x) consists of the seq_length prior values, and the output (y) is the next value.\n",
        "Data Splitting:\n",
        "\n",
        "Split the dataset into training and testing subsets using an 80:20 ratio, ensuring sufficient data for both.\n",
        "Model Design:\n",
        "\n",
        "Designed an RNN using PyTorch with one nn.RNN layer and a fully connected output layer.\n",
        "Training and Evaluation Setup:\n",
        "\n",
        "Prepared the training and testing DataLoader objects for batch processing.\n",
        "Implemented a training loop with the Mean Squared Error (MSE) loss function and Adam optimizer.\n",
        "Assumptions Made\n",
        "Sufficient Data Availability:\n",
        "\n",
        "Assumed the dataset contained at least 366 rows to accommodate a sequence length of 365 plus a target value.\n",
        "Consistent Date Format:\n",
        "\n",
        "Assumed that all dates in the dataset were formatted consistently. Adjusted the format argument dynamically based on observed discrepancies.\n",
        "Time Series Predictability:\n",
        "\n",
        "Assumed the Close values had sufficient patterns or trends for an RNN to model effectively.\n",
        "Changes Made\n",
        "Column Name Cleaning:\n",
        "\n",
        "Removed trailing spaces from column names using data.columns.str.strip().\n",
        "Date Parsing:\n",
        "\n",
        "Adjusted the format in pd.to_datetime() to handle two-digit years (%y).\n",
        "Dynamic Sequence Length:\n",
        "\n",
        "Adapted seq_length dynamically to prevent errors when the dataset was smaller than 366 rows.\n",
        "Fallback for Small Datasets:\n",
        "\n",
        "Introduced logic to handle insufficient data by either reducing the sequence length or training on the entire dataset without splitting.\n",
        "Validation of Dataset and Splits:\n",
        "\n",
        "Added checks and assertions to verify dataset length, training size, and testing size to prevent runtime errors.\n",
        "Graceful Handling of Edge Cases:\n",
        "\n",
        "Provided meaningful error messages and warnings when the dataset was too small for splitting or sequence creation."
      ],
      "metadata": {
        "id": "1oQmdR7zfFp1"
      }
    }
  ]
}